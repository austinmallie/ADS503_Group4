---
title: "ADS 503 Final Project"
author: "Austin Mallie"
date: "2025-06-11"
output: pdf_document
---

```{r}
library(readxl)
library(dplyr)
library(readr)
library(lubridate)
library(geosphere)
library(tidyr)
library(FNN)
library(tidyverse)
library(corrplot)
library(summarytools)
library(stringr)
library(caret)
```


```{r}
# Load Bleach Data
bleaching_data <- read_xlsx("Global_Coral_Bleaching_Database.xlsx")
# Load SST data
sst_data <- read_csv("sst_annual_avg_by_location.csv")

```

```{r}
# Clean column names
colnames(bleaching_data) <- make.names(colnames(bleaching_data))

# Inspect structure
str(bleaching_data)
dim(bleaching_data)
head(bleaching_data)


dfSummary(bleaching_data)

# Latitude & Longitude (Distribution of site locations)

ggplot(bleaching_data, aes(x = LATITUDE)) +
  geom_histogram(bins = 50, fill = "lightblue", color = "black") +
  labs(title = "Histogram of LATITUDE", x = "Latitude", y = "Frequency")

ggplot(bleaching_data, aes(x = LONGITUDE)) +
  geom_histogram(bins = 50, fill = "lightgreen", color = "black") +
  labs(title = "Histogram of LONGITUDE", x = "Longitude", y = "Frequency")


# YEAR

ggplot(bleaching_data, aes(x = YEAR)) +
  geom_histogram(binwidth = 1, fill = "coral", color = "black") +
  labs(title = "Histogram of YEAR", x = "Year", y = "Number of Observations")


# PERCENT_BLEACHED and MORTALITY 

# Clean Percent Bleached first:
bleaching_data <- bleaching_data %>%
  mutate(PERCENT_BLEACHED_CLEAN = suppressWarnings(as.numeric(PERCENT_BLEACHED)))


ggplot(bleaching_data, aes(x = PERCENT_BLEACHED_CLEAN)) +
  geom_histogram(bins = 50, fill = "orange", color = "black", na.rm = TRUE) +
  labs(title = "Histogram of PERCENT_BLEACHED", x = "% Bleached", y = "Frequency")

# Boxplot of PERCENT_BLEACHED 
ggplot(bleaching_data, aes(y = PERCENT_BLEACHED_CLEAN)) +
  geom_boxplot(fill = "orange", color = "black", na.rm = TRUE) +
  labs(title = "Boxplot of PERCENT_BLEACHED", y = "% Bleached")

# PERCENT_MORTALITY 
bleaching_data <- bleaching_data %>%
  mutate(PERCENT_MORTALITY_CLEAN = suppressWarnings(as.numeric(PERCENT_MORTALITY)))

ggplot(bleaching_data, aes(x = PERCENT_MORTALITY_CLEAN)) +
  geom_histogram(bins = 50, fill = "red", color = "black", na.rm = TRUE) +
  labs(title = "Histogram of PERCENT_MORTALITY", x = "% Mortality", y = "Frequency")

ggplot(bleaching_data, aes(y = PERCENT_MORTALITY_CLEAN)) +
  geom_boxplot(fill = "red", color = "black", na.rm = TRUE) +
  labs(title = "Boxplot of PERCENT_MORTALITY", y = "% Mortality")


# COUNTRY

ggplot(bleaching_data, aes(x = reorder(COUNTRY, COUNTRY, function(x)-length(x)))) +
  geom_bar(fill = "skyblue", color = "black") +
  labs(title = "Observations per Country", x = "Country", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))


```


```{r}
ggplot(bleaching_data, aes(x = PERCENT_BLEACHED_CLEAN, y = PERCENT_MORTALITY_CLEAN)) +
  geom_point(alpha = 0.3, color = "darkred") +
  labs(title = "Scatterplot: % Bleached vs % Mortality",
       x = "% Bleached", y = "% Mortality") +
  theme_minimal()

ggplot(bleaching_data, aes(x = YEAR, y = PERCENT_BLEACHED_CLEAN)) +
  geom_point(alpha = 0.3, color = "coral") +
  geom_smooth(method = "loess", color = "blue") +
  labs(title = "Scatterplot: % Bleached over Years",
       x = "Year", y = "% Bleached") +
  theme_minimal()

ggplot(bleaching_data, aes(x = YEAR, y = PERCENT_MORTALITY_CLEAN)) +
  geom_point(alpha = 0.3, color = "firebrick") +
  geom_smooth(method = "loess", color = "blue") +
  labs(title = "Scatterplot: % Mortality over Years",
       x = "Year", y = "% Mortality") +
  theme_minimal()

ggplot(bleaching_data, aes(x = LONGITUDE, y = LATITUDE)) +
  geom_point(alpha = 0.3, color = "steelblue") +
  labs(title = "Geographic Distribution of Coral Bleaching Observations",
       x = "Longitude", y = "Latitude") +
  theme_minimal()

```

In the next code block the bleach data set will be cleaned so that the column names and the values within them will be more helpful during the modeling process.

```{r}

# Clean percent columns (bleaching & mortality)
clean_percent <- function(x) {
  x <- as.character(x)
  
  if (is.na(x) || x == "N/A") return(NA_real_)
  
  x <- str_trim(x)
  x <- str_remove_all(x, "[^0-9\\-\\.]+")  # Keep digits, dash, decimal
  
  if (str_detect(x, "-")) {
    nums <- str_split(x, "-")[[1]]
    nums <- as.numeric(nums)
    return(mean(nums, na.rm = TRUE))
  }
  
  val <- suppressWarnings(as.numeric(x))
  return(val)
}

# Apply cleaning to bleaching and mortality columns
bleaching_data <- bleaching_data %>%
  mutate(
    PERCENT_BLEACHED_CLEAN = sapply(PERCENT_BLEACHED, clean_percent),
    PERCENT_MORTALITY_CLEAN = sapply(PERCENT_MORTALITY, clean_percent),
    MIN_PERCENT_BLEACHED_CLEAN = sapply(MIN_PERCENT_BLEACHED, clean_percent),
    MAX_PERCENT_BLEACHED_CLEAN = sapply(MAX_PERCENT_BLEACHED, clean_percent)
  )

```

Here the data sets will be merged on their lat and long values. Due to the differences in how the data sets handle their latitude and longitude values, grid rounding will be employed to help pair the sets together. The SST data is collected on a predefined spatial grid which often has values such as .25 degrees or 1 degree, whereas the bleaching set has precised lat and long values that do not align with the SST points. Therefore the data sets will need to be merged based on areas defined by areas that cover specific spans of Longitude and Latitude. This way geographic areas are better covered and get representation
```{r}
# Use geographic areas of waters for the bleaching set
bleaching_data <- bleaching_data %>%
  mutate(
    AREA = case_when(
      LATITUDE >= -30 & LATITUDE <= -10 & LONGITUDE >= 110 & LONGITUDE <= 160 ~ "Southwest Pacific",
    LATITUDE >= -10 & LATITUDE <= 10 & LONGITUDE >= 90 & LONGITUDE <= 180 ~ "Equatorial Pacific",
    LATITUDE >= 10 & LATITUDE <= 30 & LONGITUDE >= -80 & LONGITUDE <= -30 ~ "Caribbean",
    LATITUDE >= 10 & LATITUDE <= 30 & LONGITUDE >= 100 & LONGITUDE <= 140 ~ "South China Sea",
    LATITUDE >= -30 & LATITUDE <= 30 & LONGITUDE >= 20 & LONGITUDE <= 80 ~ "Indian Ocean",
    LATITUDE >= -40 & LATITUDE <= 40 & LONGITUDE >= -180 & LONGITUDE <= -80 ~ "Eastern Pacific",
    TRUE ~ "Other"
  ))

# Same with the SST set
sst_data <- sst_data %>%
  mutate(
    AREA = case_when(
   lat >= -30 & lat <= -10 & lon >= 110 & lon <= 160 ~ "Southwest Pacific",
    lat >= -10 & lat <= 10 & lon >= 90 & lon <= 180 ~ "Equatorial Pacific",
    lat >= 10 & lat <= 30 & lon >= -80 & lon <= -30 ~ "Caribbean",
    lat >= 10 & lat <= 30 & lon >= 100 & lon <= 140 ~ "South China Sea",
    lat >= -30 & lat <= 30 & lon >= 20 & lon <= 80 ~ "Indian Ocean",
    lat >= -40 & lat <= 40 & lon >= -180 & lon <= -80 ~ "Eastern Pacific",
    TRUE ~ "Other"
  ))

sst_area_summary <- sst_data %>%
  group_by(AREA, year) %>%
  summarise(avg_sst = median(avg_sst, na.rm = TRUE), .groups = "drop")


merged_data <- left_join(
  bleaching_data,
  sst_area_summary,
  by = c("AREA", "YEAR" = "year"))
  
# Quick check of the merged dataset
glimpse(merged_data)

```


```{r}

# Check for any missing data
colSums(is.na(merged_data))


```

Data preprocessing
```{r}
# The target of the study is not the mortality of the reefs but rather the bleaching of them. There are far too many values that are missing from this column so it will be dropped. Also columns that have no predictive value or are just metadata will be dropped as well. Min percent bleach, max percent bleach can also be dropped because they are strongly correlated with the percent bleached clean target variable
merged_data <- merged_data %>%
  select(
    -RECORD_ID,
    -SURVEY_TYPE,
    -DATA_POINT_OF_CONTACT,
    -POC_E.MAIL_ADDRESS,
    -CITATION,
    -COMMENTS,
    -PERCENT_MORTALITY,
    -PERCENT_MORTALITY_CLEAN,
    -MIN_PERCENT_BLEACHED,
    -MAX_PERCENT_BLEACHED,
    -SOURCE,
    -CORAL_REGIONS,
    -SITE_NAME
  )



```


```{r}

# Here we will clean the depth column and make it ready for modeling while also creating bins for the ranges of values depth can take.
# Clean up DEPTH column 
merged_data <- merged_data %>%
  mutate(
    DEPTH_CLEAN = as.character(DEPTH),
    DEPTH_CLEAN = ifelse(
      DEPTH_CLEAN %in% c("N/A", "NA", "", " ", "unknown", "UNK", "null", "NULL"),
      NA, DEPTH_CLEAN
    ),
    DEPTH_CLEAN = str_remove_all(DEPTH_CLEAN, "[^0-9\\.\\-]+"),
    DEPTH_CLEAN = ifelse(
      str_detect(DEPTH_CLEAN, "-"),
      sapply(str_split(DEPTH_CLEAN, "-"), function(x) {
        nums <- suppressWarnings(as.numeric(x))
        if (all(is.na(nums))) {
          return(NA_real_)
        } else {
          return(mean(nums, na.rm = TRUE))
        }
      }),
      DEPTH_CLEAN
    ),
    DEPTH_CLEAN = as.numeric(DEPTH_CLEAN)
  )

# # Since Percent_Bleached_Clean is the target variable we can drop the empty rows because it injects artificial data within the target. We cannot accurately impute the SST values without introducing bias into the model so those values will be dropped as well
merged_data_clean <- merged_data %>%
  filter(!is.na(PERCENT_BLEACHED_CLEAN)) %>%
  filter(!is.na(avg_sst))

# Group based depth imputation based on lat, long and year
merged_data_clean <- merged_data_clean %>%
  group_by(AREA, YEAR) %>%
  mutate(
    DEPTH_CLEAN = ifelse(
      is.na(DEPTH_CLEAN),
      median(DEPTH_CLEAN, na.rm = TRUE),
      DEPTH_CLEAN
    )
  ) %>%
  ungroup()

# Fall back redundancy just in case some missing values are missing, so the median depth is used in place
global_median_depth <- median(merged_data_clean$DEPTH_CLEAN, na.rm = TRUE)

merged_data_clean <- merged_data_clean %>%
  mutate(
    DEPTH_CLEAN = ifelse(
      is.na(DEPTH_CLEAN),
      global_median_depth,
      DEPTH_CLEAN
    )
  )

# Bin Depth
merged_data_clean <- merged_data_clean %>%
  mutate(
    DEPTH_BIN = case_when(
      DEPTH_CLEAN <= 5 ~ "Very Shallow",
      DEPTH_CLEAN <= 10 ~ "Shallow",
      DEPTH_CLEAN <= 20 ~ "Mid",
      DEPTH_CLEAN <= 40 ~ "Deep",
      DEPTH_CLEAN > 40 ~ "Very Deep"
    )
  )

```


```{r}
#We can now drop the depth column because all of the necessary information from in is now stored in depth_clean and Depth_bin
merged_data_clean <- merged_data_clean %>% select(-DEPTH)
```


```{r}

# Recheck for any missing data
colSums(is.na(merged_data_clean))
```

Now we can begin the modeling process by splitting the data.
```{r}
set.seed(5034)

# Select the columns you will use
model_data <- merged_data_clean %>%
  select(
    COUNTRY, LOCATION, LATITUDE, LONGITUDE, MONTH, YEAR,
    MIN_PERCENT_BLEACHED_CLEAN, MAX_PERCENT_BLEACHED_CLEAN,
    AREA, avg_sst, DEPTH_CLEAN, DEPTH_BIN, PERCENT_BLEACHED_CLEAN
  )

# Split into training and testing sets
train_index <- createDataPartition(model_data$PERCENT_BLEACHED_CLEAN, p = 0.8, list = FALSE)

train_data <- model_data[train_index, ]
test_data  <- model_data[-train_index, ]

# Check sizes
nrow(train_data)
nrow(test_data)
```


```{r}

# LINEAR REGRESSION
lm_model <- lm(PERCENT_BLEACHED_CLEAN ~ avg_sst + DEPTH_CLEAN + YEAR + MONTH, data = train_data)

# Summary of model
summary(lm_model)

# Predict on test data
lm_predictions <- predict(lm_model, newdata = test_data)

# Evaluate RMSE
lm_rmse <- sqrt(mean((lm_predictions - test_data$PERCENT_BLEACHED_CLEAN)^2, na.rm = TRUE))
cat("Linear Regression RMSE:", lm_rmse, "\n")

# Plot: Predicted vs Actual
ggplot(data = NULL, aes(x = test_data$PERCENT_BLEACHED_CLEAN, y = lm_predictions)) +
  geom_point(alpha = 0.4, color = "blue") +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Linear Regression: Predicted vs. Actual",
       x = "Actual % Bleached", y = "Predicted % Bleached") +
  theme_minimal()

```


```{r}
# LOGISTIC REGRESSION

# Create binary outcome: High Bleaching if > 50%
train_data$Bleached_High <- ifelse(train_data$PERCENT_BLEACHED_CLEAN > 50, 1, 0)
test_data$Bleached_High  <- ifelse(test_data$PERCENT_BLEACHED_CLEAN > 50, 1, 0)

# LOGISTIC REGRESSION
log_model <- glm(Bleached_High ~ avg_sst + DEPTH_CLEAN + YEAR + MONTH, 
                 data = train_data, family = "binomial")

# Summary
summary(log_model)

# Predict probabilities
log_probs <- predict(log_model, newdata = test_data, type = "response")
log_preds <- ifelse(log_probs > 0.5, 1, 0)

# Confusion matrix
library(caret)
confusionMatrix(factor(log_preds), factor(test_data$Bleached_High))

# Plot: Confusion Matrix
log_cm <- confusionMatrix(factor(log_preds), factor(test_data$Bleached_High))
fourfoldplot(log_cm$table,
             color = c("#E41A1C", "#377EB8"),
             main = "Logistic Regression: Confusion Matrix")

# ROC and AUC
library(pROC)
log_roc <- roc(test_data$Bleached_High, log_probs)
plot(log_roc, col = "blue", main = "ROC Curve - Logistic Regression")
auc(log_roc)
```


```{r}
# k-NN with k Optimization

# Normalize helper
normalize <- function(x) {
  return((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}

# Ensure MONTH is numeric
train_data$MONTH <- as.numeric(train_data$MONTH)
test_data$MONTH  <- as.numeric(test_data$MONTH)

# First: Recode target in train_data directly
train_data$Bleached_High_Factor <- factor(
  ifelse(train_data$Bleached_High == 1, "Yes", "No"),
  levels = c("No", "Yes")
)

# Then: Create knn_train_set from predictors + cleaned target
knn_train_set <- train_data %>%
  select(avg_sst, DEPTH_CLEAN, YEAR, MONTH, Bleached_High_Factor) %>%
  mutate(across(c(avg_sst, DEPTH_CLEAN, YEAR, MONTH), normalize))

# Rename for caret training
colnames(knn_train_set)[colnames(knn_train_set) == "Bleached_High_Factor"] <- "Bleached_High"

# Prepare test set (no target needed here)
test_knn <- test_data %>%
  select(avg_sst, DEPTH_CLEAN, YEAR, MONTH) %>%
  mutate(across(everything(), normalize))

# Remove any NA rows
knn_train_set <- knn_train_set %>% drop_na()

# k Optimization using caret
library(caret)

control <- trainControl(method = "cv", number = 10, classProbs = TRUE)
grid <- expand.grid(k = seq(1, 21, 2))

set.seed(503)
knn_cv_model <- train(Bleached_High ~ ., 
                      data = knn_train_set,
                      method = "knn",
                      trControl = control,
                      tuneGrid = grid)

# Results
plot(knn_cv_model)
print(knn_cv_model$bestTune)
```



```{r}
# Final k-NN Evaluation (Confusion Matrix + ROC/AUC)

# Load required libraries
library(FNN)
library(caret)
library(pROC)

# Drop NAs from training data
train_input <- knn_train_set %>% select(-Bleached_High) %>% drop_na()
train_target <- knn_train_set %>% drop_na() %>% pull(Bleached_High)

# Drop NAs from test data (predictors only)
test_input <- test_knn %>% drop_na()

# Predict using the best k from cross-validation
best_k <- knn_cv_model$bestTune$k
knn_preds <- knn(train = train_input,
                 test = test_input,
                 cl = train_target,
                 k = best_k,
                 prob = TRUE)

# Extract corresponding test labels (cleaned to match filtered predictors)
test_labels <- test_data %>%
  filter(!is.na(avg_sst) & !is.na(DEPTH_CLEAN) & !is.na(YEAR) & !is.na(MONTH)) %>%
  mutate(Bleached_High_Factor = factor(ifelse(Bleached_High == 1, "Yes", "No"), levels = c("No", "Yes"))) %>%
  pull(Bleached_High_Factor)

# Confusion matrix
confusionMatrix(knn_preds, test_labels)

# ROC and AUC
knn_prob_values <- ifelse(knn_preds == "Yes", attr(knn_preds, "prob"), 1 - attr(knn_preds, "prob"))

knn_roc <- roc(response = test_labels,
               predictor = knn_prob_values)

plot(knn_roc, col = "darkgreen", main = paste("ROC Curve - k-NN (k =", best_k, ")"))
auc(knn_roc)

```



```{r}

```



```{r}

```



```{r}

```



```{r}

```



